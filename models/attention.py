"""
æ³¨æ„åŠ›å¢å¼ºæ¨¡å—
åŒ…å«è‡ªæ³¨æ„åŠ›ï¼ˆç”¨äºå•æ¨¡æ€ç‰¹å¾å¢å¼ºï¼‰å’Œäº¤å‰æ³¨æ„åŠ›ï¼ˆç”¨äºè·¨æ¨¡æ€ç‰¹å¾å¢å¼ºï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple


def _attention_dim(hidden_size: int, num_heads: int) -> int:
    """ç”¨äºæ³¨æ„åŠ›çš„å¯æ•´é™¤ç»´åº¦ã€‚å½“ hidden_size ä¸èƒ½è¢« num_heads æ•´é™¤æ—¶ï¼Œå–æœ€å° >= hidden_size ä¸”èƒ½è¢« num_heads æ•´é™¤çš„å€¼ï¼ˆä»…æ³¨æ„åŠ›å†…éƒ¨ä½¿ç”¨ï¼Œå¯¹å¤–ä»ä¿æŒè®ºæ–‡ d_fï¼‰ã€‚"""
    if hidden_size % num_heads == 0:
        return hidden_size
    return ((hidden_size + num_heads - 1) // num_heads) * num_heads


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(
        self,
        hidden_size: int,
        num_heads: int = 8,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–å¤šå¤´æ³¨æ„åŠ›
        
        Args:
            hidden_size: éšè—å±‚å¤§å°
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            dropout: Dropoutæ¯”ç‡
        """
        super(MultiHeadAttention, self).__init__()
        assert hidden_size % num_heads == 0
        
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        # Q, K, VæŠ•å½±å±‚
        self.query_proj = nn.Linear(hidden_size, hidden_size)
        self.key_proj = nn.Linear(hidden_size, hidden_size)
        self.value_proj = nn.Linear(hidden_size, hidden_size)
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.output_proj = nn.Linear(hidden_size, hidden_size)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.head_dim)
    
    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            query: Queryå¼ é‡ [batch_size, seq_len_q, hidden_size]
            key: Keyå¼ é‡ [batch_size, seq_len_k, hidden_size]
            value: Valueå¼ é‡ [batch_size, seq_len_v, hidden_size]
            mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len_q, seq_len_k]
        
        Returns:
            output: æ³¨æ„åŠ›è¾“å‡º [batch_size, seq_len_q, hidden_size]
        """
        batch_size = query.size(0)
        
        # æŠ•å½±åˆ°Q, K, V
        Q = self.query_proj(query)  # [batch_size, seq_len_q, hidden_size]
        K = self.key_proj(key)       # [batch_size, seq_len_k, hidden_size]
        V = self.value_proj(value)   # [batch_size, seq_len_v, hidden_size]
        
        # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        # [batch_size, num_heads, seq_len, head_dim]
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        # [batch_size, num_heads, seq_len_q, seq_len_k]
        
        # åº”ç”¨æ©ç 
        if mask is not None:
            # æ‰©å±•æ©ç ç»´åº¦ä»¥åŒ¹é…å¤šå¤´
            mask = mask.unsqueeze(1).unsqueeze(1)  # [batch_size, 1, 1, seq_len_k]
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # Softmax
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attn_output = torch.matmul(attn_weights, V)
        # [batch_size, num_heads, seq_len_q, head_dim]
        
        # åˆå¹¶å¤šå¤´
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, -1, self.hidden_size)
        # [batch_size, seq_len_q, hidden_size]
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_proj(attn_output)
        
        return output


class SelfAttention(nn.Module):
    """è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ˆç”¨äºå•æ¨¡æ€ç‰¹å¾å¢å¼ºï¼‰
    æŒ‰ç…§è®ºæ–‡å…¬å¼(11)ï¼šğ‘“Ì‚ğ‘¡ = ğ‘€ğ»ğ´(ğ‘“ğ‘¡, ğ‘“ğ‘¡, ğ‘“ğ‘¡) = (ğ‘€ğ‘¡ + ğ‘“ğ‘¡) + ğ¹ğ‘ğ‘(ğ‘€ğ‘¡ + ğ‘“ğ‘¡)
    åŒ…å«ï¼šMHA + æ®‹å·® + FFN + ç¬¬äºŒæ¬¡æ®‹å·®ï¼ˆå®Œæ•´Transformer blockç»“æ„ï¼‰
    """
    
    def __init__(
        self,
        hidden_size: int,
        num_heads: int = 8,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–è‡ªæ³¨æ„åŠ›æ¨¡å—
        
        Args:
            hidden_size: éšè—å±‚å¤§å°
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            dropout: Dropoutæ¯”ç‡
        """
        super(SelfAttention, self).__init__()
        self.attention = MultiHeadAttention(hidden_size, num_heads, dropout)
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)
        self.dropout = nn.Dropout(dropout)
        
        # è®ºæ–‡å…¬å¼(11)ï¼šFFNç”±ä¸¤ä¸ªå…¨è¿æ¥ï¼ˆFCï¼‰çº¿æ€§å±‚å’ŒReLUæ¿€æ´»å‡½æ•°ç»„æˆ
        # Linear(d, 4d) -> ReLU -> Dropout -> Linear(4d, d) -> Dropout
        self.ffn = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 4, hidden_size),
            nn.Dropout(dropout)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼ˆè®ºæ–‡å…¬å¼11ï¼‰
        ğ‘“Ì‚ğ‘¡ = ğ‘€ğ»ğ´(ğ‘“ğ‘¡, ğ‘“ğ‘¡, ğ‘“ğ‘¡) = (ğ‘€ğ‘¡ + ğ‘“ğ‘¡) + ğ¹ğ‘ğ‘(ğ‘€ğ‘¡ + ğ‘“ğ‘¡)
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, hidden_size] æˆ– [batch_size, seq_len, hidden_size]
        
        Returns:
            enhanced_feat: å¢å¼ºåçš„ç‰¹å¾
        """
        # å¦‚æœè¾“å…¥æ˜¯2Dï¼Œæ·»åŠ åºåˆ—ç»´åº¦
        if x.dim() == 2:
            x = x.unsqueeze(1)  # [batch_size, 1, hidden_size]
        
        # ç¬¬ä¸€æ­¥ï¼šMHA + ç¬¬ä¸€æ¬¡æ®‹å·®è¿æ¥
        # ğ‘€ğ‘¡ = MHA(ğ‘“ğ‘¡, ğ‘“ğ‘¡, ğ‘“ğ‘¡)
        attn_output = self.attention(x, x, x)
        # (ğ‘€ğ‘¡ + ğ‘“ğ‘¡)
        x = self.norm1(x + self.dropout(attn_output))
        
        # ç¬¬äºŒæ­¥ï¼šFFN + ç¬¬äºŒæ¬¡æ®‹å·®è¿æ¥
        # ğ¹ğ‘ğ‘(ğ‘€ğ‘¡ + ğ‘“ğ‘¡)
        ffn_output = self.ffn(x)
        # (ğ‘€ğ‘¡ + ğ‘“ğ‘¡) + ğ¹ğ‘ğ‘(ğ‘€ğ‘¡ + ğ‘“ğ‘¡)
        output = self.norm2(x + ffn_output)
        
        # å¦‚æœåŸæ¥æ˜¯2Dï¼Œç§»é™¤åºåˆ—ç»´åº¦
        if output.size(1) == 1:
            output = output.squeeze(1)
        
        return output


class CrossAttention(nn.Module):
    """äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼ˆç”¨äºè·¨æ¨¡æ€ç‰¹å¾å¢å¼ºï¼‰"""
    
    def __init__(
        self,
        hidden_size: int,
        num_heads: int = 8,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–äº¤å‰æ³¨æ„åŠ›æ¨¡å—
        
        Args:
            hidden_size: éšè—å±‚å¤§å°
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            dropout: Dropoutæ¯”ç‡
        """
        super(CrossAttention, self).__init__()
        self.attention = MultiHeadAttention(hidden_size, num_heads, dropout)
        self.norm = nn.LayerNorm(hidden_size)
        self.dropout = nn.Dropout(dropout)
    
    def forward(
        self,
        query: torch.Tensor,
        key_value: torch.Tensor
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            query: Queryç‰¹å¾ [batch_size, hidden_size] æˆ– [batch_size, seq_len_q, hidden_size]
            key_value: Key-Valueç‰¹å¾ [batch_size, hidden_size] æˆ– [batch_size, seq_len_kv, hidden_size]
        
        Returns:
            enhanced_feat: å¢å¼ºåçš„ç‰¹å¾
        """
        # å¦‚æœè¾“å…¥æ˜¯2Dï¼Œæ·»åŠ åºåˆ—ç»´åº¦
        if query.dim() == 2:
            query = query.unsqueeze(1)
        if key_value.dim() == 2:
            key_value = key_value.unsqueeze(1)
        
        # äº¤å‰æ³¨æ„åŠ›ï¼šqueryæ¥è‡ªä¸€ä¸ªæ¨¡æ€ï¼Œkeyå’Œvalueæ¥è‡ªå¦ä¸€ä¸ªæ¨¡æ€
        attn_output = self.attention(query, key_value, key_value)
        
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        output = self.norm(query + self.dropout(attn_output))
        
        # å¦‚æœåŸæ¥æ˜¯2Dï¼Œç§»é™¤åºåˆ—ç»´åº¦
        if output.size(1) == 1:
            output = output.squeeze(1)
        
        return output


class CoAttentionLayer(nn.Module):
    """å•å±‚Co-Attention Transformer Block
    åŒ…å«ï¼šåŒå‘äº¤å‰æ³¨æ„åŠ› + FFN + æ®‹å·®è¿æ¥
    """
    
    def __init__(
        self,
        hidden_size: int,
        num_heads: int = 8,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–å•å±‚Co-Attention
        
        Args:
            hidden_size: éšè—å±‚å¤§å°
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            dropout: Dropoutæ¯”ç‡
        """
        super(CoAttentionLayer, self).__init__()
        
        # åŒå‘äº¤å‰æ³¨æ„åŠ›
        self.text_to_image_attn = CrossAttention(hidden_size, num_heads, dropout)
        self.image_to_text_attn = CrossAttention(hidden_size, num_heads, dropout)
        
        # FFNï¼ˆå‰é¦ˆç½‘ç»œï¼‰
        self.text_ffn = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 4, hidden_size),
            nn.Dropout(dropout)
        )
        self.image_ffn = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 4, hidden_size),
            nn.Dropout(dropout)
        )
        
        # LayerNorm
        self.text_norm = nn.LayerNorm(hidden_size)
        self.image_norm = nn.LayerNorm(hidden_size)
    
    def forward(
        self,
        text_feat: torch.Tensor,
        image_feat: torch.Tensor,
        memory_tokens: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            text_feat: æ–‡æœ¬ç‰¹å¾ [batch_size, hidden_size]
            image_feat: å›¾åƒç‰¹å¾ [batch_size, hidden_size]
            memory_tokens: å¯å­¦ä¹ å†…å­˜tokens [batch_size, memory_length, hidden_size]ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            text_enhanced: å¢å¼ºåçš„æ–‡æœ¬ç‰¹å¾ [batch_size, hidden_size]
            image_enhanced: å¢å¼ºåçš„å›¾åƒç‰¹å¾ [batch_size, hidden_size]
        """
        # åŒå‘äº¤å‰æ³¨æ„åŠ›
        # å¦‚æœæä¾›memoryï¼Œå°†å…¶ä½œä¸ºé¢å¤–çš„K/Vå‚ä¸æ³¨æ„åŠ›è®¡ç®—
        if memory_tokens is not None:
            # æ–‡æœ¬ä½œä¸ºQueryï¼šK,V = concat(image_feat, memory_tokens)
            image_with_memory = torch.cat([
                image_feat.unsqueeze(1),  # [batch_size, 1, hidden_size]
                memory_tokens  # [batch_size, memory_length, hidden_size]
            ], dim=1)  # [batch_size, 1+memory_length, hidden_size]
            
            # å›¾åƒä½œä¸ºQueryï¼šK,V = concat(text_feat, memory_tokens)
            text_with_memory = torch.cat([
                text_feat.unsqueeze(1),  # [batch_size, 1, hidden_size]
                memory_tokens  # [batch_size, memory_length, hidden_size]
            ], dim=1)  # [batch_size, 1+memory_length, hidden_size]
            
            text_attn = self.text_to_image_attn(text_feat, image_with_memory)
            image_attn = self.image_to_text_attn(image_feat, text_with_memory)
        else:
            # ä¸ä½¿ç”¨memoryçš„åŸå§‹é€»è¾‘
            text_attn = self.text_to_image_attn(text_feat, image_feat)
            image_attn = self.image_to_text_attn(image_feat, text_feat)
        
        # FFN + æ®‹å·®è¿æ¥
        text_enhanced = self.text_norm(text_attn + self.text_ffn(text_attn))
        image_enhanced = self.image_norm(image_attn + self.image_ffn(image_attn))
        
        return text_enhanced, image_enhanced


class CoAttention(nn.Module):
    """ååŒæ³¨æ„åŠ›ï¼ˆCo-Attentionï¼‰
    è®ºæ–‡ä¸­ï¼šåŒå±‚è”åˆæ³¨æ„åŠ›/ä¸¤ä¸ªCo-Attention Transformerã€‚
    å½“ hidden_size ä¸èƒ½è¢« num_heads æ•´é™¤æ—¶ï¼Œå†…éƒ¨ä½¿ç”¨ã€Œä»…ç”¨äºæ³¨æ„åŠ›çš„æŠ•å½±ç»´åº¦ã€ï¼š
    812 -> Linear -> d_attn(816) -> Co-Attention(head=8) -> Linear -> 812ï¼Œå¯¹å¤–ä»ä¸ºè®ºæ–‡ d_fã€‚
    """
    
    def __init__(
        self,
        hidden_size: int,
        num_heads: int = 8,
        dropout: float = 0.1,
        num_layers: int = 2
    ):
        """
        åˆå§‹åŒ–ååŒæ³¨æ„åŠ›
        
        Args:
            hidden_size: éšè—å±‚å¤§å°ï¼ˆè®ºæ–‡ d_f=812ï¼‰
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            dropout: Dropoutæ¯”ç‡
            num_layers: Co-Attentionå±‚æ•°ï¼ˆè®ºæ–‡è®¾ç½®ä¸º2ï¼‰
        """
        super(CoAttention, self).__init__()
        self.hidden_size = hidden_size
        d_attn = _attention_dim(hidden_size, num_heads)
        self._use_proj = d_attn != hidden_size
        if self._use_proj:
            self.proj_in = nn.Linear(hidden_size, d_attn)
            self.proj_out = nn.Linear(d_attn, hidden_size)
        # å †å å¤šå±‚Co-Attention Transformerï¼Œåœ¨ d_attn ç»´ä¸Šæ»¡è¶³ head æ•´é™¤
        self.layers = nn.ModuleList([
            CoAttentionLayer(d_attn, num_heads, dropout)
            for _ in range(num_layers)
        ])
    
    def forward(
        self,
        text_feat: torch.Tensor,
        image_feat: torch.Tensor,
        memory_tokens: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            text_feat: æ–‡æœ¬ç‰¹å¾ [batch_size, hidden_size]
            image_feat: å›¾åƒç‰¹å¾ [batch_size, hidden_size]
            memory_tokens: å¯å­¦ä¹ å†…å­˜tokens [batch_size, memory_length, hidden_size]ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            text_enhanced: å›¾åƒå¢å¼ºçš„æ–‡æœ¬ç‰¹å¾ [batch_size, hidden_size]
            image_enhanced: æ–‡æœ¬å¢å¼ºçš„å›¾åƒç‰¹å¾ [batch_size, hidden_size]
        """
        if self._use_proj:
            text_feat = self.proj_in(text_feat)
            image_feat = self.proj_in(image_feat)
            if memory_tokens is not None:
                memory_tokens = self.proj_in(memory_tokens)
        for layer in self.layers:
            text_feat, image_feat = layer(text_feat, image_feat, memory_tokens)
        if self._use_proj:
            text_feat = self.proj_out(text_feat)
            image_feat = self.proj_out(image_feat)
        return text_feat, image_feat


class FeatureEnhancementModule(nn.Module):
    """ç‰¹å¾å¢å¼ºæ¨¡å—
    æŒ‰ç…§è®ºæ–‡ï¼šä½¿ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMHSAï¼‰å¤„ç†å•æ¨¡æ€ç‰¹å¾ã€‚
    å½“ hidden_size ä¸èƒ½è¢« num_heads æ•´é™¤æ—¶ï¼Œå†…éƒ¨ä½¿ç”¨ã€Œä»…ç”¨äºæ³¨æ„åŠ›çš„æŠ•å½±ç»´åº¦ã€ï¼š
    812 -> Linear -> d_attn(816) -> MHSA(head=8) -> Linear -> 812ï¼Œå¯¹å¤–ä»ä¸ºè®ºæ–‡ d_fã€‚
    """
    
    def __init__(
        self,
        hidden_size: int,
        num_heads: int = 8,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–ç‰¹å¾å¢å¼ºæ¨¡å—
        
        Args:
            hidden_size: éšè—å±‚å¤§å°ï¼ˆè®ºæ–‡ d_f=812ï¼‰
            num_heads: æ³¨æ„åŠ›å¤´æ•°ï¼ˆè®ºæ–‡ä¸­Head=8ï¼‰
            dropout: Dropoutæ¯”ç‡
        """
        super(FeatureEnhancementModule, self).__init__()
        self.hidden_size = hidden_size
        d_attn = _attention_dim(hidden_size, num_heads)
        self._use_proj = d_attn != hidden_size
        if self._use_proj:
            self.proj_in = nn.Linear(hidden_size, d_attn)
            self.proj_out = nn.Linear(d_attn, hidden_size)
        # å•æ¨¡æ€ç‰¹å¾å¢å¼ºï¼ˆå¤šå¤´è‡ªæ³¨æ„åŠ›MHSAï¼‰ï¼Œåœ¨ d_attn ç»´ä¸Šæ»¡è¶³ head æ•´é™¤
        self.text_self_attn = SelfAttention(d_attn, num_heads, dropout)
        self.image_self_attn = SelfAttention(d_attn, num_heads, dropout)
    
    def forward(
        self,
        text_feat: torch.Tensor,
        image_feat: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            text_feat: æ–‡æœ¬ç‰¹å¾ [batch_size, hidden_size]
            image_feat: å›¾åƒç‰¹å¾ [batch_size, hidden_size]
        
        Returns:
            enhanced_text: å¢å¼ºåçš„æ–‡æœ¬ç‰¹å¾ [batch_size, hidden_size]
            enhanced_image: å¢å¼ºåçš„å›¾åƒç‰¹å¾ [batch_size, hidden_size]
        """
        if self._use_proj:
            text_feat = self.proj_in(text_feat)
            image_feat = self.proj_in(image_feat)
        enhanced_text = self.text_self_attn(text_feat)
        enhanced_image = self.image_self_attn(image_feat)
        if self._use_proj:
            enhanced_text = self.proj_out(enhanced_text)
            enhanced_image = self.proj_out(enhanced_image)
        return enhanced_text, enhanced_image
