"""
CMFFAä¸»æ¨¡å‹
è·¨æ¨¡æ€ç‰¹å¾èåˆä¸å¯¹é½çš„è™šå‡ä¿¡æ¯æ£€æµ‹æ¨¡å‹
æŒ‰ç…§è®ºæ–‡ç®—æ³•1çš„å®Œæ•´æµç¨‹å®ç°
"""

import torch
import torch.nn as nn
from typing import Dict, Optional

from transformers.modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa

from .feature_extractors import TextEncoder, ImageEncoder, CLIPEncoder, FeatureFusionModule
from .attention import FeatureEnhancementModule, CoAttention
from .ambiguity_analysis import AmbiguityAnalyzer
from .fusion import AdaptiveFusionModule
from .memory import LearnableMemory


class CMFFA(nn.Module):
    """è·¨æ¨¡æ€ç‰¹å¾èåˆä¸å¯¹é½çš„è™šå‡ä¿¡æ¯æ£€æµ‹æ¨¡å‹
    æŒ‰ç…§è®ºæ–‡ç®—æ³•1çš„æµç¨‹ï¼š
    1. ç‰¹å¾æå–ï¼ˆå¾®è§‚BERT/ResNet + å®è§‚CLIPï¼‰
    2. åˆçº§èåˆï¼ˆConcat + æŠ•å½±ï¼‰
    3. ç‰¹å¾å¢å¼ºï¼ˆMHSAï¼‰
    4. åŒæµå¤„ç†ï¼ˆCo-Attention + VAE/KLï¼‰
    5. åŠ æƒä¸æ‹¼æ¥
    6. åˆ†ç±»
    """
    
    def __init__(
        self,
        text_encoder_config: Dict,
        image_encoder_config: Dict,
        clip_config: Dict,
        fusion_config: Dict,
        attention_config: Dict,
        classifier_config: Dict
    ):
        """
        åˆå§‹åŒ–CMFFAæ¨¡å‹
        
        Args:
            text_encoder_config: æ–‡æœ¬ç¼–ç å™¨é…ç½®ï¼ˆBERTï¼‰
            image_encoder_config: å›¾åƒç¼–ç å™¨é…ç½®ï¼ˆResNetï¼‰
            clip_config: CLIPç¼–ç å™¨é…ç½®
            fusion_config: ç‰¹å¾èåˆé…ç½®
            attention_config: æ³¨æ„åŠ›æ¨¡å—é…ç½®
            classifier_config: åˆ†ç±»å™¨é…ç½®
        """
        super(CMFFA, self).__init__()
        
        # 1. ç‰¹å¾æå–å™¨ï¼ˆå¾®è§‚ç‰¹å¾ï¼‰
        self.text_encoder = TextEncoder(
            model_name=text_encoder_config.get('model_name', 'bert-base-uncased'),
            hidden_size=text_encoder_config.get('hidden_size', 768),
            dropout=text_encoder_config.get('dropout', 0.1)
        )
        
        self.image_encoder = ImageEncoder(
            model_name=image_encoder_config.get('model_name', 'resnet50'),
            pretrained=image_encoder_config.get('pretrained', True),
            hidden_size=image_encoder_config.get('hidden_size', 512),
            dropout=image_encoder_config.get('dropout', 0.1)
        )
        
        # CLIPç¼–ç å™¨ï¼ˆå®è§‚ç‰¹å¾ï¼‰
        self.clip_encoder = CLIPEncoder(
            clip_model_name=clip_config.get('model_name', 'ViT-B/16')
        )
        clip_dim = self.clip_encoder.clip_dim  # é€šå¸¸æ˜¯512
        
        # 2. ç‰¹å¾èåˆæ¨¡å—ï¼ˆå¾®è§‚+å®è§‚ï¼‰
        text_micro_dim = text_encoder_config.get('hidden_size', 768)  # BERT: 768
        image_micro_dim = image_encoder_config.get('hidden_size', 512)  # ResNet: 512
        micro_proj_dim = fusion_config.get('micro_proj_dim', 300)  # è®ºæ–‡4.3.2èŠ‚ï¼šd_l=300ï¼ˆè¯åµŒå…¥ç»´åº¦ï¼‰
        
        # è®ºæ–‡3.1.4èŠ‚ï¼šd_f = d_l + d_c = micro_proj_dim + clip_dim
        fusion_dim = micro_proj_dim + clip_dim  # 300 + 512 = 812
        
        self.feature_fusion = FeatureFusionModule(
            text_micro_dim=text_micro_dim,
            image_micro_dim=image_micro_dim,
            macro_dim=clip_dim,
            micro_proj_dim=micro_proj_dim,  # è®ºæ–‡4.3.2èŠ‚ï¼š300ç»´
            dropout=fusion_config.get('dropout', 0.1)
        )
        
        # 3. ç‰¹å¾å¢å¼ºæ¨¡å—ï¼ˆMHSAï¼‰
        # è®ºæ–‡3.2.1èŠ‚ï¼šè¾“å…¥æ˜¯èåˆç‰¹å¾f_t, f_vï¼Œç»´åº¦ä¸ºd_f=812
        # è®ºæ–‡å…¬å¼(7-11)ï¼šMHSAåœ¨d_fç»´ä¸Šæ“ä½œï¼Œè¾“å‡º\hat f_t, \hat f_vç»´åº¦ä¸å˜ï¼ˆä»ä¸ºd_fï¼‰
        self.feature_enhancement = FeatureEnhancementModule(
            hidden_size=fusion_dim,  # è®ºæ–‡d_f=812ï¼Œä¸ä½¿ç”¨å¯¹é½å±‚
            num_heads=attention_config.get('num_heads', 8),
            dropout=attention_config.get('dropout', 0.1)
        )
        
        # å¯å­¦ä¹ å†…å­˜æ¨¡å—ï¼ˆè®ºæ–‡ï¼šå¯å­¦ä¹ å†…å­˜ä¿¡æ¯çš„é•¿åº¦è®¾ç½®ä¸º50ï¼‰
        memory_length = fusion_config.get('memory_length', 50)
        self.memory = LearnableMemory(
            memory_length=memory_length,
            hidden_size=fusion_dim  # è®ºæ–‡d_f=812
        )
        
        # 4. åŒæµå¤„ç†
        # åˆ†æ”¯Aï¼šååŒæ³¨æ„åŠ›ï¼ˆCo-Attentionï¼‰
        # è®ºæ–‡å¼ºè°ƒï¼šåŒå±‚è”åˆæ³¨æ„åŠ›/ä¸¤ä¸ªCo-Attention Transformer
        # è®ºæ–‡å…¬å¼(12-14)ï¼šè¾“å…¥æ˜¯å¢å¼ºåçš„ç‰¹å¾\hat f_t, \hat f_vï¼Œç»´åº¦ä¸ºd_f=812
        self.co_attention = CoAttention(
            hidden_size=fusion_dim,  # è®ºæ–‡d_f=812
            num_heads=attention_config.get('num_heads', 8),
            dropout=attention_config.get('dropout', 0.1),
            num_layers=2  # è®ºæ–‡æ˜ç¡®ï¼šä¸¤ä¸ªCo-Attention Transformer
        )
        
        # åˆ†æ”¯Bï¼šæ­§ä¹‰æ€§åˆ†æï¼ˆVAE + KLæ•£åº¦ï¼‰
        # è®ºæ–‡å…¬å¼(15-19)ï¼šè¾“å…¥ç»´åº¦åº”ä¸ºd_fï¼ˆå¦‚æœæŒ‰æ–‡å­—æè¿°ç”¨å¢å¼ºç‰¹å¾ï¼‰æˆ–d_cï¼ˆå¦‚æœæŒ‰å…¬å¼17-18ç”¨CLIPç‰¹å¾ï¼‰
        # æœ¬å®ç°æŒ‰å…¬å¼(17)(18)ä½¿ç”¨CLIPç‰¹å¾ï¼ˆd_c=512ï¼‰ï¼Œä½†VAEç¼–ç å™¨ç»´åº¦è®¾ä¸ºd_fä»¥ä¿æŒä¸€è‡´æ€§
        self.ambiguity_analyzer = AmbiguityAnalyzer(
            hidden_size=clip_dim,  # è®ºæ–‡å…¬å¼(17)(18)ä½¿ç”¨CLIPç‰¹å¾ï¼Œç»´åº¦d_c=512
            latent_dim=fusion_config.get('latent_dim', 256)
        )
        
        # 5. è‡ªé€‚åº”èåˆæ¨¡å—
        # è®ºæ–‡å…¬å¼(20-21)ï¼šè¾“å…¥æ˜¯èåˆç‰¹å¾å’Œè·¨æ¨¡æ€ç‰¹å¾ï¼Œç»´åº¦ä¸ºd_f=812
        self.fusion_module = AdaptiveFusionModule(
            hidden_size=fusion_dim,  # è®ºæ–‡d_f=812
            dropout=fusion_config.get('dropout', 0.1)
        )
        
        # 6. åˆ†ç±»å™¨ï¼ˆè®ºæ–‡3.4èŠ‚ï¼‰
        # è®ºæ–‡3.4èŠ‚ï¼š"é™¤æœ€åä¸€å±‚å¤–ï¼Œç”±äº”ä¸ªå…·æœ‰ReLUæ¿€æ´»å‡½æ•°çš„å…¨è¿æ¥å±‚ç»„æˆ"
        # è¿™æ„å‘³ç€ï¼šå‰5å±‚FCï¼ˆå¸¦ReLUï¼‰+ æœ€å1å±‚FCï¼ˆæ— æ¿€æ´»ï¼‰= æ€»å…±6å±‚FC
        # è®ºæ–‡å…¬å¼(22)ï¼šğ‘¦Ì‚ = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ¹ğ¶ğ‘ (ğ¹))ï¼Œè¾“å‡º2 logitsç”¨äºsoftmax
        # è¾“å…¥ç»´åº¦ï¼š4 * d_f = 4 * 812 = 3248ï¼ˆè®ºæ–‡å…¬å¼21ï¼šæ‹¼æ¥F_t, F_v, F_{tv}, F_{vt}ï¼‰
        input_dim = fusion_dim * 4  # 4ä¸ªç‰¹å¾æ‹¼æ¥åçš„ç»´åº¦ï¼š4 * 812 = 3248
        dropout = classifier_config.get('dropout', 0.3)
        num_classes = classifier_config.get('num_classes', 2)
        
        # 6å±‚FCï¼šå‰5å±‚å¸¦ReLUï¼Œæœ€å1å±‚æ— æ¿€æ´»ï¼ˆè®ºæ–‡3.4èŠ‚ï¼‰
        # ç¬¬ä¸€å±‚è¾“å…¥ï¼š3248ç»´ï¼ˆè®ºæ–‡d_f * 4ï¼‰
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, fusion_dim),  # ç¬¬1å±‚ï¼š4*d_f -> d_f (3248 -> 812)
            nn.ReLU(),  # è®ºæ–‡è¦æ±‚ï¼šReLUæ¿€æ´»å‡½æ•°
            nn.Dropout(dropout),
            nn.Linear(fusion_dim, fusion_dim // 2),  # ç¬¬2å±‚ï¼š812 -> 406
            nn.ReLU(),  # è®ºæ–‡è¦æ±‚ï¼šReLUæ¿€æ´»å‡½æ•°
            nn.Dropout(dropout),
            nn.Linear(fusion_dim // 2, fusion_dim // 4),  # ç¬¬3å±‚ï¼š406 -> 203
            nn.ReLU(),  # è®ºæ–‡è¦æ±‚ï¼šReLUæ¿€æ´»å‡½æ•°
            nn.Dropout(dropout),
            nn.Linear(fusion_dim // 4, fusion_dim // 8),  # ç¬¬4å±‚ï¼š203 -> 101
            nn.ReLU(),  # è®ºæ–‡è¦æ±‚ï¼šReLUæ¿€æ´»å‡½æ•°
            nn.Dropout(dropout),
            nn.Linear(fusion_dim // 8, fusion_dim // 16),  # ç¬¬5å±‚ï¼š101 -> 50
            nn.ReLU(),  # è®ºæ–‡è¦æ±‚ï¼šReLUæ¿€æ´»å‡½æ•°
            nn.Dropout(dropout),
            nn.Linear(fusion_dim // 16, num_classes)  # ç¬¬6å±‚ï¼ˆæœ€åä¸€å±‚ï¼‰ï¼š50 -> 2ï¼ˆæ— æ¿€æ´»ï¼Œç”¨äºsoftmaxï¼‰
        )
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        images: torch.Tensor,
        text_strings: Optional[list] = None,
        return_features: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­ï¼ˆæŒ‰ç…§è®ºæ–‡ç®—æ³•1ï¼‰
        
        Args:
            input_ids: æ–‡æœ¬token IDs [batch_size, seq_len]
            attention_mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len]
            images: å›¾åƒ [batch_size, 3, H, W]
            text_strings: åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²åˆ—è¡¨ [batch_size]ï¼ˆç”¨äºCLIPç¼–ç ï¼Œå¯é€‰ï¼‰
            return_features: æ˜¯å¦è¿”å›ä¸­é—´ç‰¹å¾ï¼ˆç”¨äºåˆ†æï¼‰
        
        Returns:
            output: åŒ…å«logitså’Œå¯é€‰ç‰¹å¾çš„å­—å…¸
        """
        # 1. ç‰¹å¾æå–
        # å¾®è§‚ç‰¹å¾
        text_micro = self.text_encoder(input_ids, attention_mask)  # [batch_size, seq_len, 768]
        image_micro = self.image_encoder(images)  # [batch_size, 512]
        
        # å®è§‚ç‰¹å¾ï¼ˆCLIPï¼‰
        # CLIPéœ€è¦åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²
        if text_strings is not None:
            text_macro = self.clip_encoder.encode_text_batch(text_strings, images.device)  # [batch_size, 512]
        else:
            # å¦‚æœæ²¡æœ‰æä¾›æ–‡æœ¬ï¼Œä½¿ç”¨é›¶å‘é‡
            text_macro = torch.zeros(images.size(0), self.clip_encoder.clip_dim, device=images.device)
        
        image_macro = self.clip_encoder.encode_image(images)  # [batch_size, 512]
        
        # 2. åˆçº§èåˆï¼ˆè®ºæ–‡å…¬å¼5å’Œ6ï¼šConcatï¼‰
        # è®ºæ–‡3.1.4èŠ‚ï¼šd_f = d_l + d_c = 300 + 512 = 812
        text_fused, image_fused = self.feature_fusion(
            text_micro, text_macro, image_micro, image_macro
        )  # [batch_size, fusion_dim=812]
        
        # 3. ç‰¹å¾å¢å¼ºï¼ˆMHSAï¼‰
        # è®ºæ–‡å…¬å¼(7-11)ï¼šè¾“å…¥f_t, f_vç»´åº¦ä¸ºd_f=812ï¼Œè¾“å‡º\hat f_t, \hat f_vç»´åº¦ä¸å˜ï¼ˆä»ä¸º812ï¼‰
        text_enhanced, image_enhanced = self.feature_enhancement(
            text_fused, image_fused
        )  # [batch_size, fusion_dim=812]
        
        # è·å–å¯å­¦ä¹ å†…å­˜tokensï¼ˆè®ºæ–‡ï¼šå¯å­¦ä¹ å†…å­˜ä¿¡æ¯çš„é•¿åº¦è®¾ç½®ä¸º50ï¼‰
        batch_size = text_enhanced.size(0)
        memory_tokens = self.memory(batch_size)  # [batch_size, memory_length, hidden_size]
        
        # 4. åŒæµå¤„ç†ï¼ˆå¹¶è¡Œï¼‰
        # åˆ†æ”¯Aï¼šååŒæ³¨æ„åŠ›ï¼ˆCo-Attentionï¼‰
        # è®ºæ–‡å…¬å¼(12-14)ï¼šè¾“å…¥\hat f_t, \hat f_vç»´åº¦ä¸ºd_f=812ï¼Œè¾“å‡ºf_{tv}, f_{vt}ç»´åº¦ä¸å˜ï¼ˆä»ä¸º812ï¼‰
        # è®ºæ–‡4.3.2èŠ‚ï¼šå¯å­¦ä¹ å†…å­˜ä¿¡æ¯é•¿åº¦è®¾ç½®ä¸º50ï¼Œä½œä¸ºé¢å¤–çš„K/Vå‚ä¸æ³¨æ„åŠ›è®¡ç®—
        text_cross, image_cross = self.co_attention(
            text_enhanced, image_enhanced, memory_tokens
        )  # [batch_size, fusion_dim=812]
        
        # åˆ†æ”¯Bï¼šæ­§ä¹‰æ€§åˆ†æï¼ˆVAE + KLæ•£åº¦ï¼‰
        # è®ºæ–‡ç¬¦å·çŸ›ç›¾ï¼š
        # - æ–‡å­—æè¿°ï¼ˆå…¬å¼15-16ï¼‰ï¼šq(z_t|\hat f_t), q(z_v|\hat f_v)ï¼ˆä½¿ç”¨å¢å¼ºç‰¹å¾ï¼‰
        # - å…¬å¼(17)(18)ï¼šq(z_t|f_t^c), q(z_v|f_v^c)ï¼ˆä½¿ç”¨CLIPç‰¹å¾ï¼‰
        # æœ¬å®ç°æŒ‰å…¬å¼(17)(18)ä½¿ç”¨CLIPç‰¹å¾ï¼Œä¸å…¬å¼ä¿æŒä¸€è‡´
        ambiguity_results = self.ambiguity_analyzer(
            text_macro, image_macro  # ä½¿ç”¨CLIPç‰¹å¾f_t^c, f_v^cï¼ˆæŒ‰å…¬å¼17-18ï¼‰
        )
        
        # 5. åŠ æƒä¸æ‹¼æ¥ï¼ˆè®ºæ–‡å…¬å¼20å’Œ21ï¼‰
        # è®ºæ–‡ç®—æ³•æ­¥éª¤ï¼šå…ˆå¾—åˆ°å•æ¨¡æ€èåˆç‰¹å¾f_t, f_vï¼Œå†å¾—åˆ°å¢å¼ºåçš„\hat f_t, \hat f_v
        # å…¬å¼(20)ï¼šF_t=(1-a)f_{tuni}ï¼ŒF_v=(1-a)f_{vuni}
        # è¿™é‡Œf_{tuni}å’Œf_{vuni}æŒ‡çš„æ˜¯"å•æ¨¡æ€èåˆç‰¹å¾"ï¼Œå³èåˆåçš„text_fused/image_fused
        # æ‰€æœ‰ç‰¹å¾ç»´åº¦å‡ä¸ºd_f=812
        fused_feat = self.fusion_module(
            text_fused,  # æ–‡æœ¬å•æ¨¡æ€èåˆç‰¹å¾f_tï¼ˆè®ºæ–‡å…¬å¼20ä¸­çš„f_{tuni}ï¼‰[batch_size, 812]
            image_fused,  # å›¾åƒå•æ¨¡æ€èåˆç‰¹å¾f_vï¼ˆè®ºæ–‡å…¬å¼20ä¸­çš„f_{vuni}ï¼‰[batch_size, 812]
            text_cross,  # å›¾åƒå¢å¼ºçš„æ–‡æœ¬ç‰¹å¾f_{tv}ï¼ˆè®ºæ–‡å…¬å¼20ä¸­çš„F_{tv}ï¼‰[batch_size, 812]
            image_cross,  # æ–‡æœ¬å¢å¼ºçš„å›¾åƒç‰¹å¾f_{vt}ï¼ˆè®ºæ–‡å…¬å¼20ä¸­çš„F_{vt}ï¼‰[batch_size, 812]
            ambiguity_results
        )  # [batch_size, fusion_dim * 4 = 3248]
        
        # 6. åˆ†ç±»ï¼ˆè®ºæ–‡å…¬å¼22ï¼šğ‘¦Ì‚ = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ¹ğ¶ğ‘ (ğ¹))ï¼‰
        logits = self.classifier(fused_feat)  # [batch_size, 2]ï¼ˆ2 logitsç”¨äºsoftmaxï¼‰
        
        # æ„å»ºè¾“å‡º
        output = {
            'logits': logits,
            'probs': torch.softmax(logits, dim=-1)  # è®ºæ–‡å…¬å¼(22)ï¼šsoftmaxè¾“å‡º
        }
        
        if return_features:
            output.update({
                'text_enhanced': text_enhanced,
                'image_enhanced': image_enhanced,
                'text_cross': text_cross,
                'image_cross': image_cross,
                'fused_feat': fused_feat,
                'ambiguity': ambiguity_results
            })
        
        return output
    
    def forward_from_partial_encoder(
        self,
        bert_after_layer10: torch.Tensor,
        resnet_after_layer3: torch.Tensor,
        text_macro: torch.Tensor,
        image_macro: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        return_features: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        ä» partial ç¼“å­˜å‰å‘ï¼šåªè·‘ BERT layer 11 + projã€ResNet layer4 + projï¼Œå†èµ°èåˆâ†’åˆ†ç±»å™¨ã€‚
        bert_after_layer10: [batch_size, seq_len, 768]ï¼Œresnet_after_layer3: [batch_size, C, H, W]
        text_macro/image_macro: [batch_size, 512]ã€‚attention_mask: [batch_size, seq_len]ï¼Œç”¨äº BERT layer 11ã€‚
        """
        # BERT layer 11 + proj -> text_micro
        layer_11 = self.text_encoder.bert.encoder.layer[-1]
        if attention_mask is None:
            attention_mask = torch.ones(
                bert_after_layer10.size(0), bert_after_layer10.size(1),
                dtype=torch.long, device=bert_after_layer10.device
            )
        # BERT layer æœŸæœ› 4D attention maskï¼ˆSDPA æ ¼å¼ï¼‰ï¼Œä¸ encoder ä¸€è‡´
        extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(
            attention_mask, bert_after_layer10.dtype, tgt_len=bert_after_layer10.size(1)
        )
        layer_outputs = layer_11(bert_after_layer10, attention_mask=extended_attention_mask)
        layer_output = layer_outputs[0]  # (batch, seq_len, 768)
        text_micro = self.text_encoder.dropout(self.text_encoder.proj(layer_output))
        # ResNet layer4 + proj -> image_micro
        backbone_children = list(self.image_encoder.resnet_backbone.children())
        layer4 = backbone_children[7]
        feature_map = layer4(resnet_after_layer3)
        image_micro = self.image_encoder.proj(feature_map)
        # ä¸ forward_from_features ä¸€è‡´ï¼šèåˆ â†’ åˆ†ç±»
        text_fused, image_fused = self.feature_fusion(
            text_micro, text_macro, image_micro, image_macro
        )
        text_enhanced, image_enhanced = self.feature_enhancement(text_fused, image_fused)
        batch_size = text_enhanced.size(0)
        memory_tokens = self.memory(batch_size)
        text_cross, image_cross = self.co_attention(
            text_enhanced, image_enhanced, memory_tokens
        )
        ambiguity_results = self.ambiguity_analyzer(text_macro, image_macro)
        fused_feat = self.fusion_module(
            text_fused, image_fused, text_cross, image_cross, ambiguity_results
        )
        logits = self.classifier(fused_feat)
        output = {
            'logits': logits,
            'probs': torch.softmax(logits, dim=-1)
        }
        if return_features:
            output.update({
                'text_enhanced': text_enhanced,
                'image_enhanced': image_enhanced,
                'text_cross': text_cross,
                'image_cross': image_cross,
                'fused_feat': fused_feat,
                'ambiguity': ambiguity_results
            })
        return output
    
    def forward_from_features(
        self,
        text_micro: torch.Tensor,
        image_micro: torch.Tensor,
        text_macro: torch.Tensor,
        image_macro: torch.Tensor,
        return_features: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        ä»é¢„è®¡ç®—çš„ encoder ç‰¹å¾å‰å‘ï¼ˆè®­ç»ƒ/æ¨ç†ç”¨ç¼“å­˜æ—¶è°ƒç”¨ï¼Œä¸å†è·‘ BERT/ResNet/CLIPï¼‰
        text_micro: [batch_size, seq_len, text_micro_dim]ï¼Œimage_micro: [batch_size, image_micro_dim]
        text_macro/image_macro: [batch_size, 512]
        """
        # 2. åˆçº§èåˆ
        text_fused, image_fused = self.feature_fusion(
            text_micro, text_macro, image_micro, image_macro
        )
        # 3. ç‰¹å¾å¢å¼º
        text_enhanced, image_enhanced = self.feature_enhancement(text_fused, image_fused)
        batch_size = text_enhanced.size(0)
        memory_tokens = self.memory(batch_size)
        # 4. åŒæµ
        text_cross, image_cross = self.co_attention(
            text_enhanced, image_enhanced, memory_tokens
        )
        ambiguity_results = self.ambiguity_analyzer(text_macro, image_macro)
        # 5. åŠ æƒä¸æ‹¼æ¥
        fused_feat = self.fusion_module(
            text_fused, image_fused, text_cross, image_cross, ambiguity_results
        )
        # 6. åˆ†ç±»
        logits = self.classifier(fused_feat)
        output = {
            'logits': logits,
            'probs': torch.softmax(logits, dim=-1)
        }
        if return_features:
            output.update({
                'text_enhanced': text_enhanced,
                'image_enhanced': image_enhanced,
                'text_cross': text_cross,
                'image_cross': image_cross,
                'fused_feat': fused_feat,
                'ambiguity': ambiguity_results
            })
        return output
    
    def predict(self, input_ids, attention_mask, images, text_strings=None):
        """
        é¢„æµ‹å‡½æ•°ï¼ˆç”¨äºæ¨ç†ï¼‰
        
        Args:
            input_ids: æ–‡æœ¬token IDs
            attention_mask: æ³¨æ„åŠ›æ©ç 
            images: å›¾åƒ
            text_strings: åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            predictions: é¢„æµ‹ç±»åˆ«ï¼ˆ0æˆ–1ï¼‰
        """
        self.eval()
        with torch.no_grad():
            output = self.forward(input_ids, attention_mask, images, text_strings)
            # è®ºæ–‡å…¬å¼(22)ï¼šsoftmaxè¾“å‡ºï¼Œargmaxå¾—åˆ°é¢„æµ‹ç±»åˆ«
            logits = output['logits']  # [batch_size, 2]
            predictions = torch.argmax(logits, dim=-1)
        return predictions
