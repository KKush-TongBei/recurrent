"""
ç‰¹å¾æå–æ¨¡å—
åŒ…å«æ–‡æœ¬ç¼–ç å™¨ï¼ˆBERTï¼‰ã€å›¾åƒç¼–ç å™¨ï¼ˆResNetï¼‰å’ŒCLIPç¼–ç å™¨
æŒ‰ç…§è®ºæ–‡ï¼šå¾®è§‚ç‰¹å¾ï¼ˆBERT/ResNetï¼‰+ å®è§‚ç‰¹å¾ï¼ˆCLIPï¼‰
"""

import torch
import torch.nn as nn
from transformers import BertModel
from torchvision.models import resnet18, resnet50, ResNet18_Weights, ResNet50_Weights
import torch.nn.functional as F
from typing import Tuple
try:
    import clip
except ImportError:
    print("Warning: CLIP not installed. Please install with: pip install git+https://github.com/openai/CLIP.git")


class TextEncoder(nn.Module):
    """åŸºäºBERTçš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆå¾®è§‚ç‰¹å¾ï¼‰
    è®ºæ–‡ä¸­ï¼šå¾®è§‚ç‰¹å¾å…³æ³¨ç»†èŠ‚ï¼Œä½¿ç”¨BERTçš„last_hidden_stateåºåˆ—
    """
    
    def __init__(
        self,
        model_name: str = "bert-base-uncased",
        hidden_size: int = 768,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–æ–‡æœ¬ç¼–ç å™¨
        
        Args:
            model_name: BERTæ¨¡å‹åç§°
            hidden_size: éšè—å±‚å¤§å°
            dropout: Dropoutæ¯”ç‡
        """
        super(TextEncoder, self).__init__()
        self.bert = BertModel.from_pretrained(model_name, local_files_only=True)
        self.hidden_size = hidden_size
        self.dropout = nn.Dropout(dropout)
        
        # æŠ•å½±å±‚ï¼šå°†BERTç‰¹å¾æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        self.proj = nn.Linear(hidden_size, hidden_size)
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: è¾“å…¥token IDs [batch_size, seq_len]
            attention_mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len]
        
        Returns:
            text_feat: æ–‡æœ¬ç‰¹å¾ [batch_size, seq_len, hidden_size]
        """
        # è·å–BERTè¾“å‡ºï¼ˆè®ºæ–‡å…¬å¼1ï¼šåºåˆ—ç‰¹å¾ï¼‰
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        # ä½¿ç”¨last_hidden_stateï¼ˆåºåˆ—ç‰¹å¾ï¼‰ï¼Œåç»­ä¼šè¿›è¡ŒTokençº§æ± åŒ–
        # [batch_size, seq_len, hidden_size]
        sequence_output = outputs.last_hidden_state
        
        # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        text_feat = self.proj(sequence_output)
        text_feat = self.dropout(text_feat)
        
        return text_feat


class ImageEncoder(nn.Module):
    """åŸºäºResNetçš„å›¾åƒç¼–ç å™¨ï¼ˆå¾®è§‚ç‰¹å¾ï¼‰
    è®ºæ–‡ä¸­ï¼šå¾®è§‚ç‰¹å¾å…³æ³¨ç»†èŠ‚ï¼Œä½¿ç”¨ResNetå»æ‰FCå±‚åçš„ç‰¹å¾ï¼ˆåŒºåŸŸåºåˆ—ï¼‰
    """
    
    def __init__(
        self,
        model_name: str = "resnet50",
        pretrained: bool = True,
        hidden_size: int = 512,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–å›¾åƒç¼–ç å™¨
        
        Args:
            model_name: ResNetæ¨¡å‹åç§°
            pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            hidden_size: è¾“å‡ºéšè—å±‚å¤§å°
            dropout: Dropoutæ¯”ç‡
        """
        super(ImageEncoder, self).__init__()
        model_name_lower = (model_name or "resnet50").lower()
        # åŠ è½½ ResNetï¼ˆæ”¯æŒ resnet18 æé€Ÿã€resnet50 ä¸è®ºæ–‡ä¸€è‡´ï¼‰
        if model_name_lower == "resnet18":
            if pretrained:
                self.resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
            else:
                self.resnet = resnet18(weights=None)
            resnet_feat_size = 512  # ResNet18 æœ€åä¸€å±‚ conv è¾“å‡º 512
        else:
            if pretrained:
                self.resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
            else:
                self.resnet = resnet50(weights=None)
            resnet_feat_size = 2048  # ResNet50 æœ€åä¸€å±‚ conv è¾“å‡º 2048
        # ç§»é™¤ ResNet çš„åˆ†ç±»å±‚å’Œå…¨å±€æ± åŒ–å±‚ï¼Œä¿ç•™ç‰¹å¾å›¾
        self.resnet_backbone = nn.Sequential(*list(self.resnet.children())[:-2])
        
        # æŠ•å½±å±‚ï¼šå°†ResNetç‰¹å¾æ˜ å°„åˆ°ç›®æ ‡éšè—å±‚å¤§å°
        # è®ºæ–‡3.1.4èŠ‚ï¼šæ˜ç¡®æåˆ°"å°†æ± åŒ–çš„å›¾åƒç‰¹å¾..."
        # ResNetè¾“å‡ºæ˜¯åŒºåŸŸåºåˆ—ï¼ˆç‰¹å¾å›¾ï¼‰ï¼Œéœ€è¦æ± åŒ–ä¸ºå‘é‡
        self.proj = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),  # å…¨å±€å¹³å‡æ± åŒ–ï¼ˆè®ºæ–‡3.1.4èŠ‚ï¼‰
            nn.Flatten(),
            nn.Linear(resnet_feat_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
    
    def forward(self, images: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            images: è¾“å…¥å›¾åƒ [batch_size, 3, H, W]
        
        Returns:
            image_feat: å›¾åƒç‰¹å¾ [batch_size, hidden_size]
        """
        # é€šè¿‡ResNetæå–ç‰¹å¾å›¾ï¼ˆè®ºæ–‡å…¬å¼2ï¼šåŒºåŸŸåºåˆ—ï¼‰
        # [batch_size, 2048, H, W]
        feature_map = self.resnet_backbone(images)
        
        # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦ï¼ˆè®ºæ–‡ä¸­ä¼šè¿›è¡ŒåŒºåŸŸçº§æ± åŒ–ï¼‰
        image_feat = self.proj(feature_map)  # [batch_size, hidden_size]
        
        return image_feat


class CLIPEncoder(nn.Module):
    """CLIPç¼–ç å™¨ï¼ˆå®è§‚ç‰¹å¾ï¼‰
    è®ºæ–‡ä¸­ï¼šå®è§‚ç‰¹å¾å…³æ³¨å…¨å±€è¯­ä¹‰å¯¹é½ï¼Œä½¿ç”¨CLIP Text Encoderå’ŒImage Encoder
    """
    
    def __init__(self, clip_model_name: str = "ViT-B/16"):
        """
        åˆå§‹åŒ–CLIPç¼–ç å™¨
        
        Args:
            clip_model_name: CLIPæ¨¡å‹åç§°
        """
        super(CLIPEncoder, self).__init__()
        self.clip_model_name = clip_model_name
        self.clip_model = None
        self.clip_tokenizer = None
        self.clip_preprocess = None  # CLIPå®˜æ–¹å›¾åƒé¢„å¤„ç†
        self.clip_dim = 512  # CLIP ViT-B/16çš„è¾“å‡ºç»´åº¦æ˜¯512
        
        try:
            import clip as clip_lib
            # å»¶è¿ŸåŠ è½½ï¼Œåœ¨å®é™…ä½¿ç”¨æ—¶å†åŠ è½½åˆ°æ­£ç¡®çš„è®¾å¤‡
            self._clip_lib = clip_lib
            self._model_loaded = False
        except ImportError:
            print("Warning: CLIP not installed. Please install with: pip install git+https://github.com/openai/CLIP.git")
            self._clip_lib = None
            self._model_loaded = False
    
    def _load_model(self, device):
        """å»¶è¿ŸåŠ è½½CLIPæ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡"""
        if self._model_loaded or self._clip_lib is None:
            return
        
        try:
            self.clip_model, self.clip_preprocess = self._clip_lib.load(
                self.clip_model_name, device=device
            )
            # è·å–tokenizerï¼ˆclipåº“çš„tokenizeå‡½æ•°ï¼‰
            self.clip_tokenizer = self._clip_lib.tokenize
            self.clip_model = self.clip_model.to(device)
            self.clip_model = self.clip_model.float()  # å¼ºåˆ¶ fp32ï¼Œé¿å… MPS ä¸Š fp16/fp32 æ··ç®—è§¦å‘ dtype assert
            self.clip_dim = self.clip_model.visual.output_dim
            self._model_loaded = True
        except Exception as e:
            print(f"Warning: CLIP loading failed: {e}")
            self._model_loaded = False
    
    def encode_text(self, text: str, device: torch.device) -> torch.Tensor:
        """
        ç¼–ç æ–‡æœ¬ï¼ˆCLIP Text Encoderï¼‰
        æŒ‰ç…§è®ºæ–‡4.3.2èŠ‚ï¼šæ–‡æœ¬è¢«æˆªæ–­æˆ–å¡«å……ä¸º77ä¸ªtoken
        
        Args:
            text: æ–‡æœ¬å­—ç¬¦ä¸²ï¼ˆCLIP tokenizerä¼šè‡ªåŠ¨å¤„ç†ä¸º77 tokensï¼‰
            device: è®¾å¤‡
        
        Returns:
            text_feat: æ–‡æœ¬ç‰¹å¾ [batch_size, clip_dim]
        """
        if self._clip_lib is None:
            # å¦‚æœCLIPæœªå®‰è£…ï¼Œè¿”å›é›¶å‘é‡
            batch_size = 1 if isinstance(text, str) else text.size(0)
            return torch.zeros(batch_size, self.clip_dim, device=device)
        
        self._load_model(device)
        
        if self.clip_model is None:
            batch_size = 1 if isinstance(text, str) else text.size(0)
            return torch.zeros(batch_size, self.clip_dim, device=device)
        
        # CLIP tokenizerä¼šè‡ªåŠ¨å°†æ–‡æœ¬æˆªæ–­æˆ–å¡«å……ä¸º77ä¸ªtokenï¼ˆè®ºæ–‡4.3.2èŠ‚ï¼‰
        if isinstance(text, str):
            text_tokens = self.clip_tokenizer(text).to(device)  # [1, 77]
        else:
            # å¦‚æœå·²ç»æ˜¯tokensï¼Œç›´æ¥ä½¿ç”¨
            text_tokens = text.to(device)
        
        with torch.no_grad():
            text_feat = self.clip_model.encode_text(text_tokens)
        return text_feat.float()
    
    def encode_text_batch(self, texts: list, device: torch.device) -> torch.Tensor:
        """
        æ‰¹é‡ç¼–ç æ–‡æœ¬ï¼ˆCLIP Text Encoderï¼‰ï¼Œæ•´æ‰¹ tokenize + ä¸€æ¬¡ forwardï¼Œé¿å…é€æ¡å¾ªç¯ã€‚
        
        Args:
            texts: æ–‡æœ¬å­—ç¬¦ä¸²åˆ—è¡¨ [batch_size]
            device: è®¾å¤‡
        
        Returns:
            text_feat: æ–‡æœ¬ç‰¹å¾ [batch_size, clip_dim]
        """
        if self._clip_lib is None:
            return torch.zeros(len(texts), self.clip_dim, device=device)
        
        self._load_model(device)
        
        if self.clip_model is None:
            return torch.zeros(len(texts), self.clip_dim, device=device)
        
        # æ•´æ‰¹ tokenizeï¼ˆCLIP æˆªæ–­/å¡«å……ä¸º 77ï¼‰
        text_tokens = self.clip_tokenizer(texts, truncate=True).to(device)
        with torch.no_grad():
            text_feat = self.clip_model.encode_text(text_tokens)
        return text_feat.float()
    
    def encode_image(self, images: torch.Tensor) -> torch.Tensor:
        """
        ç¼–ç å›¾åƒï¼ˆCLIP Image Encoder - ViT-B/16ï¼‰
        çº¯ tensor æ‰¹å½’ä¸€åŒ–ï¼ˆæ•´æ‰¹ä¸€æ¬¡åšå®Œï¼‰ï¼Œé¿å… PIL å¾ªç¯æ‰“æ»¡ CPUã€æå‡ MPS ååã€‚
        è®ºæ–‡è¦æ±‚ï¼šCLIP çš„ mean/std ä¸å®˜æ–¹ä¸€è‡´ï¼›è¾“å…¥å·²æ˜¯ 224x224 æ—¶ä»…åš CLIP normalizeã€‚
        
        Args:
            images: è¾“å…¥å›¾åƒ [batch_size, 3, H, W]ï¼ˆé€šå¸¸å·² 224x224ï¼Œå¯èƒ½ ImageNet normalizeï¼‰
        
        Returns:
            image_feat: å›¾åƒç‰¹å¾ [batch_size, clip_dim]
        """
        device = images.device
        
        if self._clip_lib is None:
            return torch.zeros(images.size(0), self.clip_dim, device=device)
        
        self._load_model(device)
        
        if self.clip_model is None:
            return torch.zeros(images.size(0), self.clip_dim, device=device)
        
        # CLIP ViT-B/16 å®˜æ–¹å½’ä¸€åŒ–å‚æ•°ï¼ˆä¸ openai/CLIP ä¸€è‡´ï¼‰
        clip_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=device, dtype=images.dtype).view(1, 3, 1, 1)
        clip_std = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=device, dtype=images.dtype).view(1, 3, 1, 1)
        imagenet_mean = torch.tensor([0.485, 0.456, 0.406], device=device, dtype=images.dtype).view(1, 3, 1, 1)
        imagenet_std = torch.tensor([0.229, 0.224, 0.225], device=device, dtype=images.dtype).view(1, 3, 1, 1)
        
        # è‹¥å·²æ˜¯ ImageNet å½’ä¸€åŒ–ï¼ˆæœ‰è´Ÿå€¼ï¼‰ï¼Œå…ˆåå½’ä¸€åŒ–åˆ° [0,1]
        if images.min() < 0:
            images = images * imagenet_std + imagenet_mean
        images = torch.clamp(images, 0.0, 1.0)
        # æ•´æ‰¹åš CLIP normalizeï¼ˆä¸å®˜æ–¹ preprocess ä¸€è‡´ï¼‰
        images = (images - clip_mean) / clip_std
        
        with torch.no_grad():
            image_feat = self.clip_model.encode_image(images)
        return image_feat.float()


class FeatureFusionModule(nn.Module):
    """ç‰¹å¾èåˆæ¨¡å—
    æŒ‰ç…§è®ºæ–‡å…¬å¼5å’Œ6ï¼šå°†å¾®è§‚ç‰¹å¾ï¼ˆBERT/ResNetï¼‰å’Œå®è§‚ç‰¹å¾ï¼ˆCLIPï¼‰è¿›è¡Œæ‹¼æ¥
    è®ºæ–‡3.1.4èŠ‚ï¼šğ‘‘ğ‘“ = ğ‘‘ğ‘ + ğ‘‘ğ‘™ï¼Œğ‘‘ğ‘™æ˜¯æ–‡æœ¬/å›¾åƒæ¨¡æ€é€šè¿‡çº¿æ€§å±‚æ˜ å°„åˆ°ç›¸åŒç»´åº¦
    """
    
    def __init__(
        self,
        text_micro_dim: int,
        image_micro_dim: int,
        macro_dim: int,
        micro_proj_dim: int = 300,  # è®ºæ–‡4.3.2èŠ‚ï¼šd_l=300ï¼ˆè¯åµŒå…¥ç»´åº¦ï¼‰
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–ç‰¹å¾èåˆæ¨¡å—
        
        Args:
            text_micro_dim: æ–‡æœ¬å¾®è§‚ç‰¹å¾ç»´åº¦ï¼ˆBERTï¼Œé€šå¸¸æ˜¯768ï¼‰
            image_micro_dim: å›¾åƒå¾®è§‚ç‰¹å¾ç»´åº¦ï¼ˆResNetï¼Œé€šå¸¸æ˜¯512ï¼‰
            macro_dim: å®è§‚ç‰¹å¾ç»´åº¦ï¼ˆCLIPç»´åº¦ï¼Œé€šå¸¸æ˜¯512ï¼‰
            micro_proj_dim: å¾®è§‚ç‰¹å¾æŠ•å½±ç»´åº¦d_lï¼ˆè®ºæ–‡4.3.2èŠ‚ï¼š300ç»´è¯åµŒå…¥ï¼‰
            dropout: Dropoutæ¯”ç‡
        """
        super(FeatureFusionModule, self).__init__()
        
        # è®ºæ–‡3.1.4èŠ‚ï¼šå…ˆå°†æ–‡æœ¬/å›¾åƒæ¨¡æ€é€šè¿‡çº¿æ€§å±‚æ˜ å°„åˆ°ç›¸åŒç»´åº¦d_l
        self.text_micro_proj = nn.Linear(text_micro_dim, micro_proj_dim)
        self.image_micro_proj = nn.Linear(image_micro_dim, micro_proj_dim)
        
        # è®ºæ–‡å…¬å¼5å’Œ6ï¼šğ‘“ğ‘¡ = concat(ğ‘“ğ‘¡ğ‘¢, ğ‘“ğ‘¡ğ‘)ï¼Œğ‘“ğ‘£ = concat(ğ‘“ğ‘£ğ‘¢, ğ‘“ğ‘£ğ‘)
        # èåˆåç»´åº¦ï¼šd_f = d_l + d_c = micro_proj_dim + macro_dim
        self.fusion_dim = micro_proj_dim + macro_dim
        
        # Dropoutï¼ˆè®ºæ–‡æœªæ˜ç¡®ï¼Œä½†é€šå¸¸éœ€è¦ï¼‰
        self.dropout = nn.Dropout(dropout)
    
    def forward(
        self,
        text_micro: torch.Tensor,
        text_macro: torch.Tensor,
        image_micro: torch.Tensor,
        image_macro: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­ï¼ˆè®ºæ–‡å…¬å¼5å’Œ6ï¼‰
        è®ºæ–‡3.1.4èŠ‚ï¼šå…ˆå°†microæ˜ å°„åˆ°d_lï¼Œå†ä¸CLIP(d_c)æ‹¼æ¥å¾—åˆ°d_f=d_l+d_c
        
        Args:
            text_micro: æ–‡æœ¬å¾®è§‚ç‰¹å¾ [batch_size, seq_len, text_micro_dim] æˆ– [batch_size, text_micro_dim]
            text_macro: æ–‡æœ¬å®è§‚ç‰¹å¾ [batch_size, macro_dim]
            image_micro: å›¾åƒå¾®è§‚ç‰¹å¾ [batch_size, image_micro_dim]
            image_macro: å›¾åƒå®è§‚ç‰¹å¾ [batch_size, macro_dim]
        
        Returns:
            text_fused: èåˆåçš„æ–‡æœ¬ç‰¹å¾ [batch_size, seq_len, fusion_dim] æˆ– [batch_size, fusion_dim]
            image_fused: èåˆåçš„å›¾åƒç‰¹å¾ [batch_size, fusion_dim]
        """
        # è®ºæ–‡3.1.4èŠ‚ï¼šå…ˆå°†æ–‡æœ¬æ¨¡æ€ç‰¹å¾æ± åŒ–æˆä¸€ä¸ªç‰¹å¾å‘é‡ï¼ˆå¦‚æœæ˜¯åºåˆ—ï¼‰
        if text_micro.dim() == 3:
            # [batch_size, seq_len, text_micro_dim] -> [batch_size, text_micro_dim]
            # ä½¿ç”¨å¹³å‡æ± åŒ–ï¼ˆè®ºæ–‡3.1.4èŠ‚ï¼šTokençº§æ± åŒ–ï¼‰
            text_micro_pooled = text_micro.mean(dim=1)
        else:
            text_micro_pooled = text_micro
        
        # è®ºæ–‡3.1.4èŠ‚ï¼šé€šè¿‡çº¿æ€§å±‚å°†æ–‡æœ¬/å›¾åƒæ¨¡æ€æ˜ å°„åˆ°ç›¸åŒç»´åº¦d_l
        text_micro_proj = self.text_micro_proj(text_micro_pooled)  # [batch_size, micro_proj_dim]
        image_micro_proj = self.image_micro_proj(image_micro)  # [batch_size, micro_proj_dim]
        
        # è®ºæ–‡å…¬å¼5ï¼šğ‘“ğ‘¡ = concat(ğ‘“ğ‘¡ğ‘¢, ğ‘“ğ‘¡ğ‘)ï¼Œç»´åº¦d_f = d_l + d_c
        text_fused = torch.cat([text_micro_proj, text_macro], dim=1)  # [batch_size, micro_proj_dim + macro_dim]
        
        # è®ºæ–‡å…¬å¼6ï¼šğ‘“ğ‘£ = concat(ğ‘“ğ‘£ğ‘¢, ğ‘“ğ‘£ğ‘)ï¼Œç»´åº¦d_f = d_l + d_c
        image_fused = torch.cat([image_micro_proj, image_macro], dim=1)  # [batch_size, micro_proj_dim + macro_dim]
        
        text_fused = self.dropout(text_fused)
        image_fused = self.dropout(image_fused)
        
        return text_fused, image_fused
